---
layout: post
title: Layer
category : docs
tags : [layer ]
---
{% include JB/setup %}

<img src="{{ BASE_PATH }}/assets/image/layer.png" align="center" width="400px"/>
<span><strong>Figure 1. Base layer class</strong></span>


Layers of a neural net are represented by instances of Layer classes.
The simplified base layer class is defined as shown in Figure 1 (The full
definition is in the [API page]()). It has two fields and
two functions. The *srclayer* vector records all source layers. The
*data_* blob consists of a set of
feature vectors, e.g., one vector per image, computed from the source layers.
For a recurrent layer in RNN, the feature blob contains one vector per internal
layer. If a layer has parameters, these parameters are declared using type
*Param* which consists of a *data_* and a *grad_* blob for
parameter values and gradients respectively. The *ComputeFeature* function
evaluates the
feature blob by transforming (e.g. convolution and pooling) features from the
source layers.
*ComputeGradient* computes the gradients of parameters associated with
this layer.
These two functions are invoked by the [TrainOneBatch]({{ BASE_PATH }}/docs/train-one-batch) function
during training. SINGA has provided many
built-in layers, which can be used directly to create neural nets. Users can
also extend the layer class to implement their own feature transformation
logics as long as the two base functions are overridden to be consistent with
the *TrainOneBatch* function. Besides the common fields like name and
type, layer configuration contains some specific fields as well, e.g. file path
for data layers. The layers in SINGA are categorized as follows according to
their functionalities,

  * Data layers for loading records (e.g., images) from disk, HDFS or network into memory.
  * Parser layers for parsing features, labels, etc. from records.
  * Neuron layers for feature transformation, e.g., convolution, pooling, dropout, etc.
  * Loss layers for measuring the training objective loss, e.g., cross entropy-loss or Euclidean loss.
  * Output layers for outputting the prediction results (e.g., probabilities of each category) onto disk or network.
  * Connection layers for connecting layers when the neural net is partitioned.
