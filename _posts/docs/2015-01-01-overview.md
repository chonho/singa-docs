---
layout: post
title: Introduction
category: docs
---
{% include JB/setup %}


## Overview
SINGA is designed to be general to implement the distributed training
algorithms of existing systems. Distributed deep learning training is an on-
going challenge research problem in terms of scalability.  There is no
established scalable distributed training algorithm. Different algorithms are
used by existing systems, e.g. Hogwild used by Caffe, AllReduce used by Baidu's
DeepImage, and the Downpour algorithm proposed by Google Brain and used at
Microsoft Adam. SINGA provides users the chance to select the one that is most scalable for
their model and data.

To provide good usability, SINGA provides a simple programming model based on
the layer structure that is common in deep learning models. Users override
the base layer class to implement their own layer logics for feature
transformation. A model is constructed by configuring each layer and their
connections like Caffe. SINGA takes care of the data and model partitioning,
and makes the underlying distributed communication (almost) transparent to
users. A set of built-in layers and example models are provided.

SINGA is an [Apache incubator project](http://singa.incubator.apache.org/),
released under Apache License 2. It is mainly developed by the DBSystem group
of National University of Singapore. A diverse community is being constructed
to welcome open-source contribution.
<!--
SINGA is a distributed deep learning platform, for training large-scale deep
learning models. Our design is driven by two key observations. First, the
structures and training algorithms of deep learning models can be expressed
using simple abstractions, e.g., the layer. SINGA allows users
to write their own training algorithms by exposing intuitive programming abstractions
and hiding complex details pertaining distributed execution of the training.
Specifically, our programming model consists of data objects (layer and network)
that define the model, and of computation functions over the data objects. Our
second observation is that there are multiple approaches to partitioning the model
and the training data onto multiple machines to achieve model parallelism, data
parallelism or both. Each approach incurs different communication and synchronization
overhead which directly affects the systemâ€™s scalability. We analyze the fundamental
trade-offs of existing parallelism approaches, and propose an optimization algorithm
that generates the parallelism scheme with minimal overhead.
-->

## Goals and Principles

### Goals
* Scalability: A distributed platform that can scale to a large model and training
    dataset.
* Usability: To provide abstraction and easy to use interface so that users can
    implement their deep learning model/algorithm without much awareness of the
    underlying distributed platform.
* Extensibility: to make SINGA extensible for implementing different consistency
    models, training algorithms and deep learning models.

### Principles
Scalability is a challenge research problem for distributed deep learning
training. SINGA provides a general architecture to exploit the scalability of
different training algorithms. Different parallelism approaches are also
supported:

* Model Partition---one model replica spreads across multiple machines to handle large
    models, which have too many parameters to be kept in the memory of a single machine.
    Overhead: synchronize layer data across machines within one model replica Partition.
* Data Partition---one model replica trains against a partition of the whole training dataset.
    This approach can handle large training dataset.
    Overhead: synchronize parameters among model replicas.
* Hybrid Partition---exploit a cost model to find optimal model and data partitions which
    would reduce both overheads.

To achieve the usability goal, we propose our programming model with the following
two major considerations:

* Extract common data structures and operations for deep learning training algorithms, i.e.,
    Back Propagation and Contrastive Divergence. Users implement their models by
    inheriting these data structures and overriding the operations.
* Make model partition and data partition automatically almost transparent to
users.

Considering extensibility, we make our core data structures (e.g., Layer) and operations general enough
for programmers to override.

## System Architecture
<img src="{{ BASE_PATH }}/assets/image/arch.png" align="center" width="400px"/>
<span><strong>SINGA Logical Architecture</strong></span>

The logical system architecture is shown in the above figure. There are two
types of execution units, namely workers and servers.  They are grouped
according to the cluster configuration.  Each worker group runs against a
partition of the training dataset to compute the updates (e.g., the gradients)
of parameters on one model replica, denoted as ParamShard.  Worker groups run
asynchronously, while workers within one group run synchronously with each
worker computing (partial) updates for a subset of model parameters. Each
server group also maintains one replica of the model parameters
(i.e., ParamShard). It receives and handles requests (e.g.,
Get/Put/Update) from workers. Every server group synchronizes with neighboring server groups periodically or ac-
cording to some specified rules.

SINGA starts by parsing the cluster and model configurations. The first worker
group initializes model parameters and sends Put requests to put them into the
ParamShards of servers. Then every worker group runs the training algorithm by
iterating over its training data in mini-batch.  Each worker collects the fresh
parameters from servers before computing the updates (e.g., gradients) for
them. Once it finishes the computation, it issues update requests to the
servers.
